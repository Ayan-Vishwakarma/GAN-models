{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO+fqbblsUlPIRzObacDUTf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayan-Vishwakarma/Keras-Implementation-of-Dense-and-DC-NSGAN-WGAN-WGANGP-etc/blob/main/NSGAN_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKWgmY58B_Ax"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bigZl7UeZuXG"
      },
      "source": [
        "**Make sure either the training_data size is divisible by batch_size or the verbose in fit method is 0.Either of the above method solves the unknown error that is occuring in fit method when calling it's own callbacks due to last batch being different from the other batches [This happens when the training_data size is not divisible by batch_size].**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KF4mT4jB-avS"
      },
      "source": [
        "NSGAN ---> subclass of keras class Model\n",
        "\n",
        "NSGAN(generator,discriminator,z_dim) \n",
        "\n",
        "It requires a generator model,discriminator model and latent space dimension.\n",
        "The generator and discriminator provided need not necessarily to be compiled,uncompiled model also works.\n",
        "\n",
        "However the NSGAN model instance need to be compiled before using keras fit() method or the training_loop method.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VCTRXGn_SJk"
      },
      "source": [
        "> compile( opt_gen,opt_disc)\n",
        ">>> opt_gen :: Generator ,keras optimizer for minimizing binary_crossentropy loss.\n",
        "\n",
        ">>> opt_disc :: Discriminator, keras optimizer for minimizing binary_crossentropy loss.\n",
        "\n",
        "> training_loop( xt , n_iter , batch_size , sampling_interval )\n",
        "\n",
        ">>> xt :: The numpy ndarray in which images are stored,i.e, the training dataset. shape = (N,H,W,C) \n",
        "\n",
        ">>>n_iter :: The no of iterations to train \n",
        "\n",
        ">>>batch_size :: The batch size used for training \n",
        "\n",
        ">>>sampling_interval :: The interval after which the images are to be sampled \n",
        "\n",
        ">>>return -> discriminator loss, Generator loss, and binary accuracy of discriminator classifying ability of real and fake images after generator and discriminator training respectively.\n",
        "\n",
        "> call(inputs , with_discriminator=None)\n",
        "\n",
        ">>> inputs :: The input to be given to generator.This input is derived from latent space.\n",
        "\n",
        ">>> with_discriminator :: Boolean, If true the outputs are further passed to discriminator.Only valid if discriminator exists.\n",
        "\n",
        ">RemoveDiscriminator(self):: Removes the discriminator from the model leaving behind only the generator model for generating images.Can be used when training is complete.\n",
        "\n",
        ">SampleImages(self,x,y) :: Sample images from generator. A total of x multiplied by y images are samples displayed in x rows and y columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9MNBdzAG8bb"
      },
      "source": [
        "Attributes:\n",
        "\n",
        "generator :: The generator model for generating images\n",
        "\n",
        "discriminator :: The critic model which estimates the EM distance between the current generator's distribution and real distribution.\n",
        "\n",
        "z_dim :: Latent space dimension"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv1F9JkN2a7T"
      },
      "source": [
        "class NSGAN(keras.models.Model):\n",
        " \n",
        "  def __init__(self,generator,discriminator,z_dim,**kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.with_disc = True\n",
        "    self.generator = generator\n",
        "    self.discriminator = discriminator\n",
        "    self.z_dim = z_dim\n",
        "    self.gan = keras.Sequential([generator,discriminator])\n",
        "    self.gm = keras.metrics.BinaryAccuracy(name=\"binary_accuracy_generator\")\n",
        "    self.dm = keras.metrics.BinaryAccuracy(name=\"binary_accuracy_discriminator\")\n",
        "    assert ((self.gan.layers[0].trainable == True) and (self.gan.layers[1].trainable == True)),\"Generator and Discriminator should be trainable\"\n",
        "    \n",
        "  def call(self,inputs,with_discriminator=None):\n",
        "    if with_discriminator and self.with_disc:\n",
        "      return self.gan(inputs)      \n",
        "    else:\n",
        "      return self.generator(inputs)\n",
        " \n",
        "  def compile(self,opt_gen,opt_disc):\n",
        "    super().compile(optimizer = opt_gen,loss = \"binary_crossentropy\")\n",
        "    self.opt_gen = opt_gen\n",
        "    self.opt_disc= opt_disc\n",
        " \n",
        "  def train_step(self,imgs):\n",
        "    if isinstance(imgs,tuple):\n",
        "      imgs = data[0]\n",
        "    \n",
        "    batch_size = tf.shape(imgs)[0]\n",
        "\n",
        "    x = imgs\n",
        "    x_ = self.generator(tf.random.normal((batch_size,self.z_dim)))\n",
        "\n",
        "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
        "      xin = tf.concat([x,x_],axis=0)\n",
        "      yin = tf.concat([tf.ones((batch_size,1)),tf.zeros((batch_size,1))],axis = 0)\n",
        "      self.dm.update_state(yin,self.discriminator(xin))\n",
        "      loss = keras.losses.binary_crossentropy(yin,self.discriminator(xin))\n",
        "\n",
        "    grads = tape.gradient(loss,self.discriminator.trainable_weights)\n",
        "    self.opt_disc.apply_gradients(zip(grads,self.discriminator.trainable_weights))\n",
        "    del xin,yin,grads\n",
        " \n",
        "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
        "      xin = tf.random.normal(tf.stack([batch_size,self.z_dim],axis=0))\n",
        "      yin = tf.ones((batch_size,1))\n",
        "      self.gm.update_state(yin,self.gan(xin))\n",
        "      gloss = keras.losses.binary_crossentropy(yin,self.gan(xin))\n",
        "    \n",
        "    grads = tape.gradient(gloss,self.generator.trainable_weights)\n",
        "    self.opt_gen.apply_gradients(zip(grads,self.generator.trainable_weights))\n",
        " \n",
        "    return {\"Discriminator_loss\":loss,\"Generator_loss\":gloss,self.gm.name:self.gm.result(),self.dm.name:self.dm.result()}\n",
        " \n",
        "  def training_loop(self,xt,n_iter,batch_size,sampling_interval):\n",
        "    self.gan.compile(optimizer = self.opt_gen,loss = \"binary_crossentropy\",metrics=[\"acc\"])\n",
        "    self.discriminator.compile(optimizer = self.opt_disc,loss = \"binary_crossentropy\",metrics=[\"acc\"])\n",
        "    self.discriminator.trainable = False\n",
        "    assert self.discriminator.trainable == False\n",
        "\n",
        "    losses = []\n",
        "    accuracies = [] \n",
        "    for i in range(n_iter):\n",
        "\n",
        "      ind = np.random.randint(60000,size = batch_size)\n",
        "      w1 = self.discriminator.train_on_batch(xt[ind],np.ones((batch_size)))\n",
        "      w2 = self.discriminator.train_on_batch(self.generator(np.random.randn(batch_size,z)),np.zeros(batch_size))\n",
        "      w = np.add(w1,w2)/2\n",
        "      losses.append(w[0])\n",
        "      accuracies.append(w[1])\n",
        "\n",
        "      w = self.gan.train_on_batch(np.random.randn(batch_size,z),np.ones(batch_size))\n",
        "\n",
        "      if i%sampling_interval == 0:\n",
        "        print(i,[losses[-1],accuracies[-1]],w)\n",
        "        self.SampleImages(4,4)\n",
        "        \n",
        "    self.discriminator.trainable = True\n",
        "    return losses\n",
        "\n",
        "  def RemoveDiscriminator(self):\n",
        "    if self.with_disc == True:\n",
        "      del self.discriminator\n",
        "      self.with_disc = False\n",
        " \n",
        "  def SampleImages(self,x,y):\n",
        "    imgs = self.generator(np.random.randn(x*y,self.z_dim))\n",
        "    fig,ax = plt.subplots(x,y,figsize=(y,x))\n",
        "    if imgs.shape[-1] == 1:\n",
        "      imgs = tf.reshape(imgs,(imgs.shape[0],imgs.shape[1],imgs.shape[2]))\n",
        "      for i in range(x*y):\n",
        "        ax[i%y,i//y].imshow(imgs[i],cmap=\"gray\")\n",
        "    else:\n",
        "      for i in range(x*y):\n",
        "        ax[i%y,i//y].imshow(imgs[i],cmap=\"gray\") "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmo1nqwhBxyX"
      },
      "source": [
        "Since the Jensen Shannon loss in NSGAN model have no relation with the  convergence of gan model, that part for monitoring JensenShannon loss is skipped here. However neither the losses should become very small, neither the discriminator accuracy should reach below 0.5 and generator accuracy reach above 0.5 in the initial phase of training.This can be inferred through keras fit and training_loop's output."
      ]
    }
  ]
}